{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS7337 NLP - HW 1 \n",
    "#### David Wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**HW 1:**</u>\n",
    "\n",
    "1.Follow the instructions in chapter 1 of Bird-Klein for implementing a “lexical diversity” scoring routine. [book link](http://www.nltk.org/book/)\n",
    "\n",
    "2.Go to http://www.gutenberg.org/wiki/Children%27s_Instructional_Books_(Bookshelf), and obtain three texts (of different grade levels) from the “Graded Readers” section. Report the lexical diversity score of each. Explain whether the result was surprising.\n",
    "\n",
    "3.Also compare the vocabulary size of the same three texts. Explain whether the result was surprising. \n",
    "\n",
    "4.Write a paragraph arguing whether vocabulary size and lexical diversity in combination could be a better measure of text difficulty (or reading level) than either measure is by itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying Text to Analyze"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**3 text levels:**</u>\n",
    "- 3rd: [The Project Gutenberg EBook of McGuffey's Third Eclectic Reader](https://www.gutenberg.org/cache/epub/14766/pg14766.txt)\n",
    "     - Text: https://www.gutenberg.org/cache/epub/14766/pg14766.txt\n",
    "- 5th: [The Project Gutenberg EBook of McGuffey's Fifth Eclectic Reader](https://www.gutenberg.org/cache/epub/15040/pg15040.txt)\n",
    "    - Text: https://www.gutenberg.org/cache/epub/15040/pg15040.txt\n",
    "- 7th: [The Literary World Seventh Reader by Browne, Metcalf, and Withers](http://www.gutenberg.org/ebooks/19721)\n",
    "    - Text: http://www.gutenberg.org/ebooks/19721"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Creating Text Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from urllib import request\n",
    "from nltk import word_tokenize\n",
    "import re #regular expressions to clean up\n",
    "\n",
    "#3rd\n",
    "text1_url = \"https://www.gutenberg.org/cache/epub/14766/pg14766.txt\"\n",
    "#5th\n",
    "text2_url = \"https://www.gutenberg.org/cache/epub/15040/pg15040.txt\"\n",
    "#7th\n",
    "text3_url = \"https://www.gutenberg.org/cache/epub/19721/pg19721.txt\"\n",
    "\n",
    "#combined\n",
    "text_urls = [\"https://www.gutenberg.org/cache/epub/14766/pg14766.txt\",\n",
    "            \"https://www.gutenberg.org/cache/epub/15040/pg15040.txt\",\n",
    "            \"https://www.gutenberg.org/cache/epub/19721/pg19721.txt\",]\n",
    "\n",
    "# reads in electronic book\n",
    "def getRaw(text):\n",
    "    url = text\n",
    "    response = request.urlopen(url)\n",
    "    raw = response.read().decode('utf8')\n",
    "    raw_character_count = len(raw)\n",
    "    return raw, raw_character_count\n",
    "\n",
    "# gets the title of book by searching for each group based on the 'Title:' keyword\n",
    "def getTitle(text):\n",
    "    raw = getRaw(text)[0]\n",
    "    search_title = re.search('Title:(.*)', raw) #finds entire group\n",
    "    title = search_title.group(1).replace(\"\\r\", ' ').strip()\n",
    "    return title\n",
    "\n",
    "# converts token into NLKT text object for linguistic processing\n",
    "def getNLKTText(text):\n",
    "    raw = getRaw(text)[0]\n",
    "    # tokenization to break up string into words and punctuation\n",
    "    tokens = word_tokenize(raw)\n",
    "    text = nltk.Text(tokens)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Lexical Diversity & Vocabulary Size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is lexical diversity? Lexical words are words such as nouns, adjectives, verbs, and adverbs that convey meaning in a text. They’re the words that you’d expect a child to use when first learning to speak. For example, ‘cat’ ‘play’ and ‘red’. [Source](https://textinspector.com/help/lexical-diversity/#:~:text=What%20is%20Lexical%20Diversity%3F,convey%20meaning%20in%20a%20text.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexical_diversity(my_text_data):\n",
    "    word_count = len(my_text_data)\n",
    "    vocab_size = len(set(my_text_data))\n",
    "    diversity_score = vocab_size / word_count\n",
    "    return word_count, vocab_size, diversity_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[37993, 4713, 0.12404916695180691], [126615, 14290, 0.1128618252181811], [128420, 13062, 0.10171312879613767]]\n",
      "================================================================================\n",
      "[168159, 605278, 621668]\n",
      "================================================================================\n",
      "[\"McGuffey's Third Eclectic Reader\", \"McGuffey's Fifth Eclectic Reader\", 'The Literary World Seventh Reader']\n",
      "Wall time: 3.79 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# create empty list to store text lexical diversity summaries\n",
    "lexical_diversity_total = []\n",
    "raw_counts = []\n",
    "book_titles = []\n",
    "\n",
    "# loop through each text url, convert to NLKT text object format and then create lexical summary\n",
    "for url in text_urls:\n",
    "    nlkt_text = getNLKTText(url)\n",
    "    # retrivies original raw string count\n",
    "    raw_count = getRaw(url)[1]\n",
    "    # retrieves book title\n",
    "    book_title = getTitle(url)\n",
    "    lexical_summary = list(lexical_diversity(nlkt_text))\n",
    "    \n",
    "    # append to empty lexical diversity summary\n",
    "    lexical_diversity_total.append(lexical_summary)\n",
    "    # appends original raw count\n",
    "    raw_counts.append(raw_count)\n",
    "    # append to empty book title \n",
    "    book_titles.append(book_title)\n",
    "    \n",
    "print(lexical_diversity_total)\n",
    "print('='*80)\n",
    "print(raw_counts)\n",
    "print('='*80)\n",
    "print(book_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyzing Lexical Diversity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin our analysis, we will start by converting our list of lexical diversity summaries into a manageable dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   tokenized_word_count  vocab_size  diversity_score\n",
      "0                 37993        4713         0.124049\n",
      "1                126615       14290         0.112862\n",
      "2                128420       13062         0.101713\n",
      "                          book_title\n",
      "0   McGuffey's Third Eclectic Reader\n",
      "1   McGuffey's Fifth Eclectic Reader\n",
      "2  The Literary World Seventh Reader\n",
      "================================================================================\n",
      "                          book_title  raw_word_count  tokenized_word_count  \\\n",
      "0   McGuffey's Third Eclectic Reader          168159                 37993   \n",
      "1   McGuffey's Fifth Eclectic Reader          605278                126615   \n",
      "2  The Literary World Seventh Reader          621668                128420   \n",
      "\n",
      "   vocab_size  diversity_score  \n",
      "0        4713         0.124049  \n",
      "1       14290         0.112862  \n",
      "2       13062         0.101713  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#create df from list\n",
    "lexical_diversity_total_df = pd.DataFrame(lexical_diversity_total, \n",
    "                                            columns=['tokenized_word_count', 'vocab_size', 'diversity_score'])\n",
    "raw_counts_df = pd.DataFrame(raw_counts, columns=['raw_word_count'])\n",
    "book_title_df = pd.DataFrame(book_titles, columns=['book_title'])\n",
    "\n",
    "print(lexical_diversity_total_df)\n",
    "print(book_title_df)\n",
    "print('='*80)\n",
    "\n",
    "#join in our book names\n",
    "final_df_list = [book_title_df, raw_counts_df, lexical_diversity_total_df]\n",
    "lexical_summary_df = pd.concat(final_df_list,axis=1)\n",
    "print(lexical_summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a paragraph arguing whether vocabulary size and lexical diversity in combination could be a better measure of text difficulty (or reading level) than either measure is by itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
