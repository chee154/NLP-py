{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DS7337 NLP - HW 3\n",
    "#### David Wei"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3\n",
    "\n",
    "<u>**HW 3:**</u>\n",
    "\n",
    "[book link](http://www.nltk.org/book/)\n",
    "\n",
    "1. Compare your given name with your nickname (if you donâ€™t have a nickname, invent one for this assignment) by answering the following questions:\n",
    "\n",
    "    - What is the edit distance between your nickname and your given name?\n",
    "    - What is the percentage string match between your nickname and your given name?\n",
    "Show your work for both calculations.\n",
    "\n",
    "2. Find a friend (or family member or classmate) who you know has read a certain book. Without your friend knowing, copy the first two sentences of that book. Now rewrite the words from those sentences, excluding stop words. Now tell your friend to guess which book the words are from by reading them just that list of words. Did you friend correctly guess the book on the first try? What did he or she guess? Explain why you think you friend either was or was not able to guess the book from hearing the list of words. \n",
    "\n",
    "3. Run one of the stemmers available in Python. Run the same two sentences from question 2 above through the stemmer and show the results. How many of the outputted stems are valid morphological roots of the corresponding words? Express this answer as a percentage.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from urllib import request\n",
    "import matplotlib as plt\n",
    "import os\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import requests\n",
    "import urllib.request\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1, Pt 1 - Edit Distance of Name and Nickname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, we will first compare my nickname, \"DWei\" which my family calls me and my given name, \"David\". Using the Levenshtein edit-distance between the nickname and given name, we'll look to find the number of characters that need to be substituted, inserted, or deleted, to transform s1 into s2.\n",
    "\n",
    "Source: https://tedboy.github.io/nlps/generated/generated/nltk.edit_distance.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "edit distance:  5\n"
     ]
    }
   ],
   "source": [
    "nick_name = \"DWei\"\n",
    "given_name = \"David Wei\"\n",
    "\n",
    "edit_distance = nltk.edit_distance(nick_name, given_name, transpositions=False)\n",
    "print('edit distance: ',edit_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1, Pt 2 - Percentage String Match of Name and Nickname"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will next calculate the percentage string match between the nickname and the given name. To do this, we will continue to utilize the Levenshtein Distance which is a metric to measure how apart two sequences of words are. The percentage string match will simply be the ratio between the 2 strings given their distance as shown in the equation below:\n",
    "\n",
    "![title](https://i0.wp.com/predictivehacks.com/wp-content/uploads/2020/12/image-3.png?resize=422%2C158&ssl=1)\n",
    "\n",
    "Source: https://www.datacamp.com/community/tutorials/fuzzy-string-python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def levenshtein_ratio_and_distance(s, t, ratio_calc = False):\n",
    "    \"\"\" levenshtein_ratio_and_distance:\n",
    "        Calculates levenshtein distance between two strings.\n",
    "        If ratio_calc = True, the function computes the\n",
    "        levenshtein distance ratio of similarity between two strings\n",
    "        For all i and j, distance[i,j] will contain the Levenshtein\n",
    "        distance between the first i characters of s and the\n",
    "        first j characters of t\n",
    "    \"\"\"\n",
    "    # Initialize matrix of zeros\n",
    "    rows = len(s)+1\n",
    "    cols = len(t)+1\n",
    "    distance = np.zeros((rows,cols),dtype = int)\n",
    "\n",
    "    # Populate matrix of zeros with the indeces of each character of both strings\n",
    "    for i in range(1, rows):\n",
    "        for k in range(1,cols):\n",
    "            distance[i][0] = i\n",
    "            distance[0][k] = k\n",
    "\n",
    "    # Iterate over the matrix to compute the cost of deletions,insertions and/or substitutions    \n",
    "    for col in range(1, cols):\n",
    "        for row in range(1, rows):\n",
    "            if s[row-1] == t[col-1]:\n",
    "                cost = 0 # If the characters are the same in the two strings in a given position [i,j] then the cost is 0\n",
    "            else:\n",
    "                # In order to align the results with those of the Python Levenshtein package, if we choose to calculate the ratio\n",
    "                # the cost of a substitution is 2. If we calculate just distance, then the cost of a substitution is 1.\n",
    "                if ratio_calc == True:\n",
    "                    cost = 2\n",
    "                else:\n",
    "                    cost = 1\n",
    "            distance[row][col] = min(distance[row-1][col] + 1,      # Cost of deletions\n",
    "                                 distance[row][col-1] + 1,          # Cost of insertions\n",
    "                                 distance[row-1][col-1] + cost)     # Cost of substitutions\n",
    "    if ratio_calc == True:\n",
    "        # Computation of the Levenshtein Distance Ratio\n",
    "        Ratio = ((len(s)+len(t)) - distance[row][col]) / (len(s)+len(t))\n",
    "        return Ratio\n",
    "    else:\n",
    "        # print(distance) # Uncomment if you want to see the matrix showing how the algorithm computes the cost of deletions,\n",
    "        # insertions and/or substitutions\n",
    "        # This is the minimum number of edits needed to convert string a to string b\n",
    "        return \"The strings are {} edits away\".format(distance[row][col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "61.53846153846154%\n"
     ]
    }
   ],
   "source": [
    "# normalize name inputs by assigning lowercase value to them\n",
    "distance = levenshtein_ratio_and_distance(nick_name.lower(), given_name.lower(), ratio_calc=True )\n",
    "print(str(distance*100)+'%')"
   ]
  },
  {
   "source": [
    "We can see that my nickname and my given name are roughly only 62% similar. This could be that the last name portion of my nickname 'Wei' is also contained in the full given name. However, because the nickname abbreviates my given names first name it cannot find any way to transform it back which accounts for the the other 38%.  "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q2 - Guessing text without stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Start of text:  8507\nEnd of text:  9593\n"
     ]
    }
   ],
   "source": [
    "# import text\n",
    "tale_of_2_cites = \"https://www.gutenberg.org/files/98/98-h/98-h.htm\"\n",
    "\n",
    "# reads in electronic book\n",
    "def getRaw(text):\n",
    "    url = text\n",
    "    response = request.urlopen(url)\n",
    "    raw = response.read().decode('utf8')\n",
    "    normalized_text = raw.lower()\n",
    "    raw_character_count = len(raw)\n",
    "    return raw, raw_character_count, normalized_text\n",
    "\n",
    "# retrieving raw text from book\n",
    "raw_text = getRaw(tale_of_2_cites)[0]\n",
    "\n",
    "# return first 1 sentence locations\n",
    "start_text = raw_text.find(\"It was the best of times, it was the worst of times\")\n",
    "end_text = raw_text.find(\"It was the year of Our Lord one thousand seven hundred and seventy-five\")\n",
    "print('Start of text: ',str(start_text))\n",
    "print('End of text: ',str(end_text))\n",
    "\n",
    "# extracting only the first 2 sentences and cleaning it\n",
    "text = raw_text[start_text:end_text].replace('\\r\\n', '').replace('</p>','').replace('<p>', '')\n",
    "\n",
    "# removing punctuation\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "text_cleaned = tokenizer.tokenize(text)\n",
    "\n",
    "# turning cleaned tokenized text back to string format\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "text_nonalpha = TreebankWordDetokenizer().detokenize(text_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"It best times worst times age wisdom age foolishness epoch belief epoch incredulity season Light season Darkness spring hope winter despair everything us nothing us going direct Heaven going direct way mdash short period far like present period noisiest authorities insisted received good evil superlative degree comparison There king large jaw queen plain face throne England king large jaw queen fair face throne France In countries clearer crystal lords State preserves loaves fishes things general settled ever \"\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "word_tokens = word_tokenize(text_nonalpha)\n",
    " \n",
    "# # filtering out stopwords from text\n",
    "filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "filtered_sentence = []\n",
    " \n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "        filtered_sentence.append(' ')\n",
    "        full_filtered_sentece=''.join(filtered_sentence)\n",
    "\n",
    "# return first 10 words from text with stop words removed\n",
    "print('\"'+str(full_filtered_sentece)+'\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After filtering out all the stop words from the the original text, I had my girlfriend read the paragraph above without any prior context other than to guess what the book was. Knowing that she had read the classic Charle's Dicken book 'A Tale of Two Cities' before in her childhood, I was surprised when she off the bat did not know what book she was reading from. My guess is that inflection matters and the stop words add a sort of linguistic poetry as the words roll of the tongue that helps readers recall information as they recite it. In this case of the famous book intro \"It **was** the best **of** times, it **was** the worst **of** times\", both 'was' and 'of' gave the text a sort of poetic feel to it and without it, would be hard to distinguish. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q3 - Percentage Match of text using Stemming\n",
    "\n",
    "Stemming is the process of reducing inflection in words to their root forms even if the stem itself is not a valid word in the language. To find how similar the outputted stems are to their valid morphological root, we will reruse the Levenshtein edit-distance ratio to compare the before and after similarity of our text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "def stemText(text):\n",
    "    stem_text=[]\n",
    "    for word in text:\n",
    "        stem_text.append(ps.stem(word))\n",
    "        stem_text.append(\" \")\n",
    "    return \"\".join(stem_text)\n",
    "\n",
    "stemmed_text_nostop = stemText(filtered_sentence)\n",
    "stemmed_text = stemText(word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "% match with stopwords removed: 7.2727272727272725%\n% match with stemming only: 6.23608017817372%\n"
     ]
    }
   ],
   "source": [
    "# normalize name inputs by assigning lowercase value to them\n",
    "distance_stemmed_nostop = levenshtein_ratio_and_distance(stemmed_text_nostop.lower(), tale_of_2_cites.lower(), ratio_calc=True )\n",
    "distance_stemmed = levenshtein_ratio_and_distance(stemmed_text.lower(), tale_of_2_cites.lower(), ratio_calc=True )\n",
    "print('% match with stopwords removed:',str(distance_stemmed_nostop*100)+'%')\n",
    "print('% match with stemming only:',str(distance_stemmed*100)+'%')"
   ]
  },
  {
   "source": [
    "To do our stemming, we will utilize Porter's Stemmer, which comparitively to it's other stemming counterparts (ex. Snowball, Lancaster) provides the least aggresive algorithm in it's stemming approach. Using the Porter's Stemmer as our 'best case' scenario stemming, we then ran the stemmer through 2 text groups: one with stop words removed and the other with only the original tokenized text stemmed. We can see that based on our results, the overall similarity between our stemmed text and it's morphological original text was drastically different. However, we also observed that the stemmed text with it's stopwords removed resulted in a slightly higher percentage match, this could be due to how including stop words resulted in non-valid terms and by removing them entirely more matching."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python385jvsc74a57bd03e05b179a6f1b3d43dc78f183e30163ca03b692d154be2549bbe3fabbd792f43",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}