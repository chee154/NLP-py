{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "f4fde45515710cbe4f4cf44a8ddef1b298277709bd6c5462499553af68a98f2e"
   }
  },
  "interpreter": {
   "hash": "3e05b179a6f1b3d43dc78f183e30163ca03b692d154be2549bbe3fabbd792f43"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### DS7337 NLP - HW 4\n",
    "#### David Wei"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Homework4\n",
    "\n",
    "<u>**HW 4:**</u>\n",
    "\n",
    "[book link](http://www.nltk.org/book/)\n",
    "\n",
    "1.\tRun one of the part-of-speech (POS) taggers available in Python. \n",
    "    - a. Find the longest sentence you can, longer than 10 words, that the POS tagger tags correctly. Show the input and output.\n",
    "    - b. Find the shortest sentence you can, shorter than 10 words, that the POS tagger fails to tag 100 percent correctly. Show the input and output. Explain your conjecture as to why the tagger might have been less than perfect with this sentence.\n",
    "\n",
    "2.\tRun a different POS tagger in Python. Process the same two sentences from question 1.\n",
    "    - a. Does it produce the same or different output?\n",
    "    - b. Explain any differences as best you can.\n",
    "\n",
    "3.\tIn a news article from this weekâ€™s news, find a random sentence of at least 10 words.\n",
    "    - a. Looking at the Penn tag set, manually POS tag the sentence yourself.\n",
    "    - b. Now run the same sentences through both taggers that you implemented for questions 1 and 2. Did either of the taggers produce the same results as you had created manually?\n",
    "    - c. Explain any differences between the two taggers and your manual tagging as much as you can.\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U textblob\n",
    "#!pip install requests\n",
    "#!pip install bs4\n",
    "# !pip install spacy\n",
    "# !pip install -U varname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python\n",
    "import os\n",
    "import numpy as np\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "from urllib import request\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "# nltk\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "# nltk corpus\n",
    "from nltk.corpus import brown\n",
    "# POS taggers\n",
    "from textblob import TextBlob\n",
    "import spacy\n",
    "# viz\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "import matplotlib as plt\n",
    "# sklearn\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "# data mine\n",
    "from bs4 import BeautifulSoup\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "# of words in long_sentence: 70\n# of words in short_setence: 4\n"
     ]
    }
   ],
   "source": [
    "##### Global Variables #####\n",
    "toktok = ToktokTokenizer()\n",
    "\n",
    "long_sentence = '''It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of light, it was the season of darkness, it was the spring of hope, it was the winter of despair.'''\n",
    "short_sentence = '''please start with me'''\n",
    "short_sentence2 = '''start please with me'''\n",
    "\n",
    "print(\"# of words in long_sentence: \"+str(len(toktok.tokenize(long_sentence))))\n",
    "print(\"# of words in short_setence: \"+str(len(toktok.tokenize(short_sentence))))\n",
    "\n",
    "# storing tokenzed variables in-memory\n",
    "long_token = toktok.tokenize(long_sentence)\n",
    "short_token = toktok.tokenize(short_sentence)\n",
    "short_token2 = toktok.tokenize(short_sentence2)"
   ]
  },
  {
   "source": [
    "## Using NLTK pos_tag"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('please', 'VB'), ('start', 'NN'), ('with', 'IN'), ('me', 'PRP')]\n================================================================================\n[('start', 'JJ'), ('please', 'NN'), ('with', 'IN'), ('me', 'PRP')]\n"
     ]
    }
   ],
   "source": [
    "long_POS = nltk.pos_tag(long_token)\n",
    "short_POS = nltk.pos_tag(short_token)\n",
    "short_POS2 = nltk.pos_tag(short_token2)\n",
    "# print(long_POS)\n",
    "# print('='*80)\n",
    "print(short_POS)\n",
    "print('='*80)\n",
    "print(short_POS2)"
   ]
  },
  {
   "source": [
    "For this example, I utilized my prior corpus example from Charles Dicken's classic novel, 'A Tale of Two Cities', the intro sentence to his book is one that is very famous and renowned as it not only long but is also poetic in explaining the duality of the curernt political and social climate at the time. Running the NLTK pos_tagger that NLTK (by default) recommends as it's out of the box tagger, it seems to capture most of the word tokens accurately as each comma separated intro of \"it was the ...\" all accurately are labeled it's PRP, VD and DT labels. \n",
    "\n",
    "Comparing a shorter text, it took a few tries to find a sentence that the NLTK pos_tagger incorrectly identified and couldn't seem to find one myself so I found one online that a user was asking about. It appeared that in this case, the pos_tagger was reading in 'please' as a noun though in reality, it could be used as an adverb (RB), verb (FB) or interjection (UH). My guess why the tagger is recogning the 'please' as a noun is that it's order in the sentence comes after the noun 'start' which coincidently is also incorrectly labeled as an adjective and thus becomes the \"subject\" of the sentence as the parser doesn't recognize the re-ordering.\n",
    "\n",
    "\n",
    "Source: https://stackoverflow.com/questions/35737099/why-is-pos-tag-in-nltk-tagging-please-as-nn"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Testing with TextBlob and spaCy"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[('please', 'VB'), ('start', 'NN'), ('with', 'IN'), ('me', 'PRP')]\n================================================================================\n[('start', 'JJ'), ('please', 'NN'), ('with', 'IN'), ('me', 'PRP')]\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "long_POS_textbob = TextBlob(long_sentence)\n",
    "short_POS_textbob = TextBlob(short_sentence)\n",
    "short_POS2_textbob = TextBlob(short_sentence2)\n",
    "# print(long_POS)\n",
    "# print('='*80)\n",
    "print(short_POS_textbob.tags)\n",
    "print('='*80)\n",
    "print(short_POS2_textbob.tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[(please, 'UH'), (start, 'VB'), (with, 'IN'), (me, 'PRP')]\n================================================================================\n[(start, 'VB'), (please, 'UH'), (with, 'IN'), (me, 'PRP')]\n"
     ]
    }
   ],
   "source": [
    "# python -m spacy download en_core_web_sm\n",
    "spacey = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# function to extract spacy objects\n",
    "def getPOS(sentence):\n",
    "    words = [i for i in sentence]\n",
    "    pos_tag = [i.tag_ for i in sentence]\n",
    "    pair = list(zip(words, pos_tag))\n",
    "    return pair\n",
    "\n",
    "long_POS_spacey = spacey(long_sentence)\n",
    "short_POS_spacey = spacey(short_sentence)\n",
    "short_POS2_spacey = spacey(short_sentence2)\n",
    "# print(long_POS)\n",
    "# print('='*80)\n",
    "print(getPOS(short_POS_spacey))\n",
    "print('='*80)\n",
    "print(getPOS(short_POS2_spacey))"
   ]
  },
  {
   "source": [
    "We can see that after testing our incorrectly identified short sentence, we next test the same short sentence using 2 different POS taggers: TextBlob and spaCy. We can see that based on the POS tagged results, TextBlob performs very similary to the NLTK pos_tag as there is virtually no difference between the two based on our simple test. However, utilizing the spaCy pos tagger, we can see that identifies the issues found in both prior taggers. After further researech to the differences between NLTK and spaCy we found that many experts in the field consider spaCy the \"industrial strength\" python NLP library that is geared towards performance. From a POS tagging perspective, there is evidence showing how spaCy outperforms NLTK when it comes to POS-tagging at the word tokenization level but not the sentence tokenization level. This is due to how NLTK splits the text at a sentence level whereas spaCy constructs a syntactic tree for each sentence. \n",
    "\n",
    "Source: https://medium.com/@akankshamalhotra24/introduction-to-libraries-of-nlp-in-python-nltk-vs-spacy-42d7b2f128f2"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Comparing NLTK vs spaCy with latest news snippet\n",
    "\n",
    "News article: https://finance.yahoo.com/news/state-crypto-congressional-hearings-ramping-133000991.html\n",
    "\n",
    "In this next section, we will use some of the latest news in the crypto-currency market and run a sentence through both our POS taggers (NLTK, spaCy) and compare the results. I chose to use crypto as the primary news as the verbage is considered \"modern\" to see how up to date some of the dictionaries both NLTK and spaCy uses. \n",
    "\n",
    "For our testing purpose we will observe the sentence below:\n",
    "> \"It feels like Congress is starting to look a bit more closely at crypto\"\n",
    "\n",
    "To begin, we will first manually do a POS tag on the sentence:\n",
    "\n",
    "> ('It', 'PRP'), ('feels', 'VBZ'), ('like', 'IN'), ('Congress', 'NNP'), ('is', 'VBZ'), ('starting', 'VBG'), ('to', 'TO'), ('look', 'VB'), ('a', 'DT'), ('bit', 'NN'), ('more', 'RBR'), ('closely', 'RB'), ('at', 'IN'), ('crypto', 'NN')\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_sentence_1 = '''It feels like Congress is starting to look a bit more closely at crypto'''\n",
    "news_sentence_2 = '''Last week, U.S. Sen. Elizabeth Warren (D-Mass.), a former presidential contender and a longstanding advocate for consumer protections, hosted a Senate subcommittee hearing on cryptocurrencies.'''\n",
    "news = [news_sentence_1, news_sentence_2]\n",
    "\n",
    "\n",
    "nltk_pos = []\n",
    "spacy_pos = []\n",
    "\n",
    "def pos_compare(show_results=False):\n",
    "    num = 0\n",
    "    for i in news:\n",
    "        num +=1\n",
    "        # returns token word count\n",
    "        news_token = toktok.tokenize(i)\n",
    "        \n",
    "        # POS tag each sentence\n",
    "        nltk_pos_result = nltk.pos_tag(news_token)\n",
    "        spacey_pos_result = getPOS(spacey(i))\n",
    "\n",
    "        nltk_pos.append(nltk_pos_result)\n",
    "        spacy_pos.append(spacey_pos_result)\n",
    "        \n",
    "        # printing results parameter (set to False for final)\n",
    "        if (show_results == True):\n",
    "            print('news_sentence_'+str(num)+':')\n",
    "            print('# of words: ',len(news_token))\n",
    "            print('nltk pos_tag: \\n'+str(nltk_pos))\n",
    "            print('\\n')\n",
    "            print('spaCy: \\n'+str(spacy_pos))\n",
    "            print('='*100)\n",
    "        else:\n",
    "            if (show_results in (False, None)): pass\n",
    "            else: \n",
    "                if (show_results not in (True, False, None)): \n",
    "                    raise ValueError('wrong parameter provided')\n",
    "    return nltk_pos, spacy_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_results = pos_compare(show_results=False) # suppressing results for final writeup"
   ]
  },
  {
   "source": [
    "Comparing my manually tagged sentence along with both the NLTK pos_tagger and the spaCy tagger, we can observe that all the tagged tokenized words are matching and accurate. What's uncertain is that though 'crypto' is natually a noun, but both taggers labeled 'crypto' also as a noun, but where I'm unsure about is if that's because both taggers do not know what 'crypto' is and thus defaulting it as a noun or if it is actually correctly tagging that word.\n",
    "\n",
    "Since the first sentence was not quite provideing \"different\" results, I pulled another sentence to test (news_sentece_2) to test with. Though most POS tagged parts were once again the same, the following interestingly was differet.\n",
    "\n",
    "NLTK:\n",
    "> ('D-Mass.', 'NNP')\n",
    "\n",
    "spaCy:\n",
    "> (D, 'NNP'), (-, 'HYPH'), (Mass., 'NNP')\n",
    "\n",
    "We can see that spaCy as part of it's pipeline, the default tokenizer is a bit more granular as it reads D.Mass (District of Massachusetts) separately as opposed to the single element the NLTK tokenizer uses. However aside from different tokenization methods, both POS taggers provide similar results from the 2 sentence comparison tested here.\n",
    "\n",
    "spaCy pipeline documentation: https://spacy.io/usage/processing-pipelines"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}